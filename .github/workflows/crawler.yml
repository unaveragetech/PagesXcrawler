name: Web Crawler

on:
  issues:
    types: [opened, edited]

jobs:
  crawl:
    runs-on: ubuntu-latest
    if: contains(github.event.issue.title, ':depth(')
    
    steps:
    - uses: actions/checkout@v2
      with:
        token: ${{ secrets.MY_PAT }}

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Parse issue title
      run: |
        issue_title="${{ github.event.issue.title }}"
        issue_number="${{ github.event.issue.number }}"
        
        # Function to add comment to the issue
        add_comment() {
          curl -X POST \
            -H "Authorization: token ${{ secrets.MY_PAT }}" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/issues/$issue_number/comments" \
            -d "{\"body\":\"$1\"}"
        }

        # First try to match with params pattern
        if [[ "$issue_title" =~ ^(https?://[^:]+):depth\(([0-9]+)\)(:params\(([^)]+)\))?$ ]]; then
            url="${BASH_REMATCH[1]}"
            depth="${BASH_REMATCH[2]}"
            params="${BASH_REMATCH[4]}"
            
            # Set default values
            max_pages=100
            timeout=10
            rotate_agent=10
            requests_per_second=2
            
            # Parse parameters if they exist
            if [ ! -z "$params" ]; then
                IFS=',' read -ra PARAM_ARRAY <<< "$params"
                for param in "${PARAM_ARRAY[@]}"; do
                    key="${param%%=*}"
                    value="${param#*=}"
                    case "$key" in
                        "max-pages") max_pages="$value" ;;
                        "timeout") timeout="$value" ;;
                        "rotate-agent-after") rotate_agent="$value" ;;
                        "requests-per-second") requests_per_second="$value" ;;
                    esac
                done
            fi
            
            # Export all variables
            {
                echo "URL=$url"
                echo "DEPTH=$depth"
                echo "MAX_PAGES=$max_pages"
                echo "TIMEOUT=$timeout"
                echo "ROTATE_AGENT=$rotate_agent"
                echo "REQUESTS_PER_SECOND=$requests_per_second"
                echo "ISSUE_TITLE=$issue_title"
                echo "ISSUE_NUMBER=$issue_number"
            } >> $GITHUB_ENV
            
            # Add configuration comment
            add_comment "📋 Configuration:\n\`\`\`\nURL: $url\nDepth: $depth\nMax Pages: $max_pages\nTimeout: ${timeout}s\nRotate Agent: Every $rotate_agent requests\nRequests/Second: $requests_per_second\n\`\`\`"
        else
            error_msg="❌ Error: Invalid issue title format.\n\nPlease use one of these formats:\n\`\`\`\nurl:depth(n)\nurl:depth(n):params(param1=value1,param2=value2)\n\`\`\`\n\nExamples:\n\`\`\`\nhttps://example.com:depth(3)\nhttps://example.com:depth(2):params(max-pages=50,timeout=15)\n\`\`\`"
            add_comment "$error_msg"
            exit 1
        fi

    - name: Run crawler
      run: |
        echo "Running crawler with URL: ${{ env.URL }}"
        python crawler.py "${{ env.URL }}" "${{ env.DEPTH }}" \
          --max-pages "${{ env.MAX_PAGES }}" \
          --timeout "${{ env.TIMEOUT }}" \
          --rotate-agent-after "${{ env.ROTATE_AGENT }}" \
          --requests-per-second "${{ env.REQUESTS_PER_SECOND }}"

    - name: Update visualization
      run: python scripts/update_html.py

    - name: Update issue status
      run: |
        echo "${{ env.ISSUE_TITLE }},${{ env.ISSUE_NUMBER }},completed" >> data/issues_status.csv

    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/* index.html
        git commit -m "Update data and visualization from issue #${{ env.ISSUE_NUMBER }}"
        git push

    - name: Close issue
      run: |
        # Add completion comment
        completion_msg="✅ Crawl completed! View results:\n- [Dashboard](https://unaveragetech.github.io/PagesXcrawler/)\n- [CSV Data](https://github.com/${{ github.repository }}/blob/main/data/results.csv)\n- [JSON Data](https://github.com/${{ github.repository }}/blob/main/data/results.json)"
        
        curl -X POST \
          -H "Authorization: token ${{ secrets.MY_PAT }}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/issues/${{ env.ISSUE_NUMBER }}/comments" \
          -d "{\"body\":\"$completion_msg\"}"
        
        curl -X PATCH \
          -H "Authorization: token ${{ secrets.MY_PAT }}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/issues/${{ env.ISSUE_NUMBER }}" \
          -d '{"state":"closed"}'
