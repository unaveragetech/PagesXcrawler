name: Web Crawler

on:
  issues:
    types: [opened, edited]
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to crawl'
        required: true
      depth:
        description: 'Crawl depth'
        required: true
        default: '2'
      max_pages:
        description: 'Maximum pages to crawl'
        required: false
        default: '100'
      timeout:
        description: 'Request timeout in seconds'
        required: false
        default: '10'
      rotate_agent:
        description: 'Rotate user agent after N requests'
        required: false
        default: '10'

jobs:
  crawl:
    runs-on: ubuntu-latest
    if: contains(github.event.issue.title, 'crawl:') || github.event_name == 'workflow_dispatch'

    permissions:
      contents: write
      issues: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.MY_PAT }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas pyyaml matplotlib PyPDF2 reportlab
        pip install --upgrade html5lib lxml

    - name: Parse inputs
      id: parse-inputs
      run: |
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          {
            echo "URL=${{ inputs.url }}"
            echo "DEPTH=${{ inputs.depth }}"
            echo "MAX_PAGES=${{ inputs.max_pages }}"
            echo "TIMEOUT=${{ inputs.timeout }}"
            echo "ROTATE_AGENT=${{ inputs.rotate_agent }}"
            echo "ISSUE_NUMBER=0"
          } >> "$GITHUB_ENV"
        else
          issue_title="${{ github.event.issue.title }}"
          issue_number="${{ github.event.issue.number }}"

          post_comment() {
            curl -X POST \
              -H "Authorization: token ${{ secrets.MY_PAT }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/issues/$issue_number/comments" \
              -d "{\"body\":\"$1\"}"
          }

          if [[ "$issue_title" =~ ^crawl:[[:space:]]*(https?://[^[:space:]]+)[[:space:]]+([0-9]+)(.*)$ ]]; then
            url="${BASH_REMATCH[1]}"
            depth="${BASH_REMATCH[2]}"
            options="${BASH_REMATCH[3]}"

            max_pages=100
            timeout=10
            rotate_agent=10

            if [ ! -z "$options" ]; then
              [[ "$options" =~ --max-pages[[:space:]]+([0-9]+) ]] && max_pages="${BASH_REMATCH[1]}"
              [[ "$options" =~ --timeout[[:space:]]+([0-9]+) ]] && timeout="${BASH_REMATCH[1]}"
              [[ "$options" =~ --rotate-agent-after[[:space:]]+([0-9]+) ]] && rotate_agent="${BASH_REMATCH[1]}"
            fi

            {
              echo "URL=$url"
              echo "DEPTH=$depth"
              echo "MAX_PAGES=$max_pages"
              echo "TIMEOUT=$timeout"
              echo "ROTATE_AGENT=$rotate_agent"
              echo "ISSUE_NUMBER=$issue_number"
            } >> "$GITHUB_ENV"

            config_msg="üìã Crawl Configuration:\n\`\`\`\nURL: $url\nDepth: $depth\nMax Pages: $max_pages\nTimeout: ${timeout}s\nRotate Agent: $rotate_agent\n\`\`\`"
            post_comment "$config_msg"
          else
            error_msg="‚ùå Invalid issue title format.\n\nUse:\n\`\`\`\ncrawl: URL DEPTH [--max-pages N] [--timeout N] [--rotate-agent-after N]\n\`\`\`\nExample:\n\`\`\`\ncrawl: https://example.com 3 --max-pages 50 --timeout 15\n\`\`\`"
            post_comment "$error_msg"
            exit 1
          fi
        fi

    - name: Run crawler
      run: |
        echo "Starting crawl on: ${{ env.URL }}"
        python crawler.py "${{ env.URL }}" "${{ env.DEPTH }}" \
          --max-pages "${{ env.MAX_PAGES }}" \
          --timeout "${{ env.TIMEOUT }}" \
          --rotate-agent-after "${{ env.ROTATE_AGENT }}"

    - name: Generate visualizations and report
      run: |
        {
          python scripts/generate_visualizations.py
          python scripts/update_html.py

          timestamp=$(date +"%Y%m%d_%H%M%S")
          mkdir -p data/archive
          cp data/results.csv "data/archive/results_${timestamp}.csv"
          cp data/results.json "data/archive/results_${timestamp}.json"
        } || {
          echo "‚ùå Visualization step failed."
          if [ "${{ env.ISSUE_NUMBER }}" != "0" ]; then
            errlog=$(tail -n 50 /home/runner/work/${{ github.repository }}/PagesXcrawler/scripts/generate_visualizations.py 2>&1 | sed 's/"/\\"/g')
            curl -X POST \
              -H "Authorization: token ${{ secrets.MY_PAT }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/issues/${{ env.ISSUE_NUMBER }}/comments" \
              -d "{\"body\":\"‚ö†Ô∏è Error during visualization:\n\`\`\`log\n$errlog\n\`\`\`\"}"
          fi
          exit 1
        }

    - name: Archive crawl data
      uses: actions/upload-artifact@v4
      with:
        name: crawl-data-${{ env.ISSUE_NUMBER }}-${{ github.run_id }}
        path: |
          data/results.*
          data/archive/
          visualizations/
          index.html

    - name: Update crawl history
      run: |
        mkdir -p data/history
        timestamp=$(date +"%Y-%m-%d %H:%M:%S")
        pages_crawled=$(wc -l < data/results.csv)
        echo "$timestamp,${{ env.URL }},${{ env.DEPTH }},${{ env.MAX_PAGES }},$pages_crawled,completed" >> data/history/crawl_history.csv

    - name: Commit and push results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/* visualizations/* index.html
        git commit -m "üîÑ Update crawl data [run #${{ github.run_id }}]"
        git push

    - name: Post crawl summary to issue
      if: env.ISSUE_NUMBER != '0'
      run: |
        pages_crawled=$(wc -l < data/results.csv)
        summary="‚úÖ Crawl completed!\n\n**Results**:\n- Pages Crawled: $pages_crawled\n- Depth: ${{ env.DEPTH }}\n- Max Pages: ${{ env.MAX_PAGES }}\n\nüîó [Dashboard](https://${{ github.repository_owner }}.github.io/${{ github.repository_name }}/)\nüìÑ [CSV](https://github.com/${{ github.repository }}/blob/main/data/results.csv)\nüßæ [PDF](https://github.com/${{ github.repository }}/blob/main/visualizations/report_${{ github.run_id }}.pdf)\n\n_Completed at: $(date)_"

        curl -X POST \
          -H "Authorization: token ${{ secrets.MY_PAT }}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/issues/${{ env.ISSUE_NUMBER }}/comments" \
          -d "{\"body\":\"$summary\"}"

        curl -X PATCH \
          -H "Authorization: token ${{ secrets.MY_PAT }}" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/issues/${{ env.ISSUE_NUMBER }}" \
          -d '{"state":"closed"}'
